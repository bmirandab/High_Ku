{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "buG6CR47RRwz",
    "outputId": "54a6f604-8ab7-4aac-d4e6-849258da94c9"
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge/label/cf201901 pyphen -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "S5McC2RjuzZi",
    "outputId": "59e7c923-eecc-45b4-dd74-506d33c9d15b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/BeatrizMiranda/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "import keras\n",
    "import pyphen\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.corpus import words, wordnet\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uA5nH5719x1o"
   },
   "source": [
    "**Importing Haiku Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRM7rXOWuzZ1"
   },
   "outputs": [],
   "source": [
    "text = open('haikus_all3.csv', encoding=\"latin-1\").read()\n",
    "regex = re.compile(r\"[\\n\\r\\t]\")\n",
    "regex2 = re.compile(r\"[^\\w\\d'\\s\\ +]\")\n",
    "text = regex.sub(\" \", text)\n",
    "text = regex2.sub(\" \", text)\n",
    "text = re.sub(' +', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JsxNcHMV94Sj"
   },
   "source": [
    "**Prepping Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0XOyhsLBuzZ5",
    "outputId": "3890af0c-aa80-4383-a91a-a83234afcced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 249013\n",
      "Unique characters: 70\n"
     ]
    }
   ],
   "source": [
    "seq_len = 50\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - seq_len, step):\n",
    "    sentences.append(text[i: i + seq_len])\n",
    "    next_chars.append(text[i + seq_len])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cKF4DKvOJ6u"
   },
   "outputs": [],
   "source": [
    "n_chars = len(text)\n",
    "n_vocab = len(chars)\n",
    "n_sentences = len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8npAgisXEJHK"
   },
   "source": [
    "**Vectorizing Haikus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07voWwGitjsk"
   },
   "outputs": [],
   "source": [
    "x = np.zeros((n_sentences, seq_len, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sentences, n_vocab), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KKrhbdHEM7q"
   },
   "source": [
    "**Creating Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_EK80eIurXD"
   },
   "outputs": [],
   "source": [
    "filepath=\"lstm4_weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "filepath_dir = os.path.dirname(filepath)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
    "                             save_best_only=True, mode='min')\n",
    "callbacks_list2 = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1AuVadgKESwN"
   },
   "source": [
    "**Creating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "X1w4rYS6uzZ9",
    "outputId": "2f3c15b4-38b5-4571-b99a-21b40d84eab8"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(seq_len, n_vocab)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_vocab, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9ri-X3z9kuy"
   },
   "source": [
    "**Loading Model from Specific Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ieobh9RT5NrX"
   },
   "outputs": [],
   "source": [
    "filepath_current = \"lstm4_weights-improvement-01-1.0284.hdf5\"\n",
    "model.load_weights(filepath_current)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8VbyQoU9qdB"
   },
   "source": [
    "**Generating Haikus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5AEPKmfuzaE"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    np.seterr(divide = 'ignore') \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OFim-twzuzaI",
    "outputId": "1f62b85b-5f97-4e1d-f7db-a093c8bddb29"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-85597ffa7871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chars\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mhaiku\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "start_index = random.randint(0, n_chars - seq_len - 1)\n",
    "generated_text = text[start_index: start_index + seq_len]\n",
    "\n",
    "for temperature in [0.4]:\n",
    "  haiku = []\n",
    "  for i in range(100):\n",
    "      sampled = np.zeros((1, seq_len, n_vocab))\n",
    "      for t, char in enumerate(generated_text):\n",
    "          sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "      preds = model.predict(sampled, verbose=0)[0]\n",
    "      next_index = sample(preds, temperature)\n",
    "      next_char = chars[next_index]\n",
    "\n",
    "      generated_text += next_char\n",
    "      generated_text = generated_text[1:]\n",
    "\n",
    "      haiku.append(next_char)\n",
    "\n",
    "  haiku_gen = \"\".join(haiku)\n",
    "  print(haiku_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C991OWR6YhUk"
   },
   "source": [
    "**Filtering Non-English Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LKfd8MgQP9gI",
    "outputId": "eaf0c6b2-9dfe-4345-f435-c629904a8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little', 'butterfly', 'with', 'a', 'field', 'the', 'temple', 'flitting', 'first', 'month', 'the', 'head', 'field', 'of', 'snow']\n"
     ]
    }
   ],
   "source": [
    "haiku = haiku_gen.split()\n",
    "en_haiku = [w for w in haiku if w in words.words()]\n",
    "print(en_haiku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EW79Jzr1Y8Pn"
   },
   "source": [
    "**Splitting Words into Syllables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "82qBWz_W_itt",
    "outputId": "43413f15-b721-47b0-9283-74d2418fbb4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lit-tle', 'but-ter-fly', 'with', 'a', 'field', 'the', 'tem-ple', 'flit-ting', 'first', 'month', 'the', 'head', 'field', 'of', 'snow']\n"
     ]
    }
   ],
   "source": [
    "dic = pyphen.Pyphen(lang='en')\n",
    "haiku_syllables =[]\n",
    "\n",
    "haiku_syllables = [dic.inserted(w) for w in en_haiku]\n",
    "print(haiku_syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "ffJ62B8bPOlO",
    "outputId": "0f487c1b-fbe0-470d-c5d6-d1f1e87dea4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lit-tle', 2],\n",
       " ['but-ter-fly', 3],\n",
       " ['with', 1],\n",
       " ['a', 1],\n",
       " ['field', 1],\n",
       " ['the', 1],\n",
       " ['tem-ple', 2],\n",
       " ['flit-ting', 2],\n",
       " ['first', 1],\n",
       " ['month', 1],\n",
       " ['the', 1],\n",
       " ['head', 1],\n",
       " ['field', 1],\n",
       " ['of', 1],\n",
       " ['snow', 1]]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllables=[]\n",
    "for w in haiku_syllables:\n",
    "  syllables_count = w.split('-')\n",
    "  syllables.append([w, len(syllables_count)])\n",
    "\n",
    "syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "52Ll_iuydHR2",
    "outputId": "71a61ab7-b5e7-44b7-b58c-9e7c785b452e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little butterfly\n",
      "with a field the temple first\n",
      "flitting month the head\n"
     ]
    }
   ],
   "source": [
    "line_count = {\"line1\": 0, \"line2\": 0, \"line3\": 0}\n",
    "haiku_final = {\"line1\": [], \"line2\": [], \"line3\": []}\n",
    "\n",
    "\n",
    "for w in syllables:\n",
    "#   if w[0] == :\n",
    "#       continue\n",
    "  if w[1] + line_count[\"line1\"] <= 5:  \n",
    "    haiku_final[\"line1\"].append(w[0])\n",
    "    line_count[\"line1\"] = w[1] + line_count[\"line1\"]\n",
    "  elif w[1] + line_count[\"line2\"] <= 7:\n",
    "    haiku_final[\"line2\"].append(w[0])\n",
    "    line_count[\"line2\"] = w[1] + line_count[\"line2\"]\n",
    "  elif w[1] + line_count[\"line3\"] <= 5:\n",
    "    haiku_final[\"line3\"].append(w[0])\n",
    "    line_count[\"line3\"] = w[1] + line_count[\"line3\"]\n",
    "\n",
    "lines = [\" \".join(haiku_final['line1']), \" \".join(haiku_final['line2']), \" \".join(haiku_final['line3']) ]\n",
    "haiku_printable = \"\\n\".join(lines)\n",
    "haiku_printable = haiku_printable.replace('-', '')\n",
    "print(haiku_printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LzUaBSbFgpW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "split_haiku_LSTM3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
